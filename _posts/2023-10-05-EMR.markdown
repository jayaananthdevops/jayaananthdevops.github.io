---
title: "Continuous Data Ingestion with Snowpipe and Stream in Snowflake"
date: 2023-10-15 12:00:00 +/-0800
categories: [Snowflake]
tags: [snowpipe,stream]]     # TAG names should always be lowercase
---

**Introduction**
Snowflake data pipelines offer significant benefits to organisations by streamlining and automating data processing workflows. Snowflake pipelines revolutionise data management and empower organisations to derive valuable insights from their data assets in a more efficient and timely manner.

In this blog, weâ€™ll explain how to create a Snowflake data pipeline to automate the manual processes involved in creating and managing ELT logic for transforming and improving continuous data loads.

**Definitions**

**Snowpipe**
Snowpipe is a cloud-native, real-time data ingestion service provided by Snowflake. It allows you to load data into Snowflake tables automatically as new data arrives in your cloud storage (e.g., Amazon S3 or Azure Blob Storage).

**Change Data Capture**
Change Data Capture (CDC) is a technique used to capture and propagate changes made to a database in real-time. CDC identifies and captures data changes, such as inserts, updates, and deletes, from a source system's transaction log. It provides a reliable and efficient way to track data changes and replicate them to target systems.

**Stream**
A stream records data manipulation language (DML) changes made to a table, directory table, external table, or the underlying tables in a view (including secure views). The object for which changes are recorded is called the source object.

**Architecture**
The following diagram represents the architecture of the Snowflake data pipeline.
