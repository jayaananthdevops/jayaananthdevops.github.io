[
  
  {
    "title": "Efficient XML Data Processing and Querying with AWS Glue and Athena - A Comprehensive Guide",
    "url": "/posts/aws_xml_athena/",
    "categories": "AWS Athena",
    "tags": "glue, athena, xml, glue catalog, glue crawler",
    "date": "2024-03-11 12:00:00 +1100",
    





    
    "snippet": "IntroductionIn today’s data-driven landscape, the analysis of XML files holds significant importance across diverse industries such as finance, healthcare, and government. The utilization of XML da...",
    "content": "IntroductionIn today’s data-driven landscape, the analysis of XML files holds significant importance across diverse industries such as finance, healthcare, and government. The utilization of XML data facilitates organizations in gaining valuable insights, enhancing decision-making processes, and streamlining data integration efforts. However, the semi-structured and highly nested nature of XML files presents challenges in accessing and analyzing information, particularly in the case of large and complex schemas.In this blog, we will delve into the process of reading XML files in a tabular format using Amazon Athena, leveraging AWS Glue for cataloging, classification, and Parquet-based processing. This comprehensive guide will provide insights into effectively handling XML data within the AWS environment, enabling seamless querying and analysis using Athena.PrerequisitesBefore you begin this tutorial, complete the following prerequisites:  Download the sample files from kaggle site  Establish an S3 Bucket and upload the sample files to it.  Set up all the essential IAM roles and policies.  Generate an AWS Database utilizing Glue.Steps to processs the XML file(i) Before creating a crawler we need to create an xml classifier where we define the rowtagAWS Classifier(ii) Generate an AWS Glue crawler to extract metadata from XML files, then execute the crawler to generate a table in the AWS Glue Data Catalog.AWS Glue CatalogAWS Glue Database(iii) Go to Athena to view the table and query itIf we try to query the XML table the query will be failed since Athena wont support XML formatQuery XML Table Athena(iv) Generate a Glue job to convert XML files to the Parquet format, which can be accomplished using two different methods.      Method 1- Use Visual editor from Glue anc convert from XML to Parquet format      Method 2 - Use script editor and write the script. In this example I have used pysparkMethod 1:Step 1 : Go to AWS Glue and select ETL Jobs -&gt; Visual ETLStep 2 : Choose AWS Glue Data Catalog as Source and select your respective table as below                                    Glue Visual Editor SourceStep 3 : Under transformation choose Change Schema.Step 4 : Under target select S3 bucket ,parquet format and snappy compression as below                 Glue Visual Editor TargetStep 5 : Choose the respective IAM Role and Run the Glue JobMethod 2:Step 1 : Go to AWS Glue and select ETL Jobs -&gt; Visual ETL -&gt; Script editorStep 2 : Copy paste the below code and run the jobimport sysfrom awsglue.transforms import *from awsglue.utils import getResolvedOptionsfrom pyspark.context import SparkContextfrom awsglue.context import GlueContextfrom awsglue.job import Jobfrom awsglue.dynamicframe import DynamicFrame# Initialize GlueContextsc = SparkContext.getOrCreate()glueContext = GlueContext(sc)spark = glueContext.spark_sessionjob = Job(glueContext)# Define input and output pathsinput_path = \"s3://glue-xml-file/\"output_path = \"s3://glue-parquet-file/\"# Read XML data as DynamicFramexml_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = \"glue-etl\", table_name = \"xml_glue_xml_file\")# Convert specific data types to string using Spark SQL functionsxml_dynamic_frame_df = xml_dynamic_frame.toDF()# Write data to Parquet formatglueContext.write_dynamic_frame.from_options(frame = DynamicFrame.fromDF(xml_dynamic_frame_df, glueContext, \"transformed_df\"), connection_type = \"s3\", connection_options = {\"path\": output_path}, format = \"parquet\",mode=\"overwrite\")# Commit the jobjob.commit()(v) Create an AWS Glue crawler to extract Parquet metadata and run the crawler which will create a table in the AWS Glue Data Catalog (Choose the same classifier that we created in the first steps)AWS Crawler Parquet SourceAWS Crawler Parquet Output(vi) Query the table in Athena and the result will be in the JSON formatQuery Parquet Table Using Athena(vii) To display the JSON data in a tabular format, we must unnest the column. It is essential to grasp the table’s schema before proceeding with the unnesting process.from pyspark.sql import SparkSession# Create a SparkSessionspark = SparkSession.builder \\    .appName(\"Read Parquet File from S3\") \\    .getOrCreate()# S3 path where the Parquet file is locateds3_path = \"s3a://glue-parquet-file/\"# Read the Parquet file into a DataFramedf = spark.read.parquet(s3_path)# Print the schema of the DataFramedf.printSchema()# Stop the SparkSessionspark.stop()Schema Structure of the TableSchema Structure Of Parquet TableUnnesting the Table using SQLTo query the table with nested structureSELECT     con.contributorrole,    nid.idtypename,    conFROM     \"glue-etl\".\"parquet_glue_parquet_file\"CROSS JOIN     UNNEST(contributor) AS t(con)CROSS JOIN     UNNEST(con.NameIdentifier) AS t(nid);Unnesting the nested Table using SQLConclusionIn this article, we have explored the process of reading an XML file using Athena and flattening the table structure for easier analysis. This approach enables users to validate the source data prior to further processing. In our next publication, we will delve into the detailed steps involved in processing XML files using AWS Glue, enhancing the understanding and utilization of these powerful AWS services.If you enjoy the article, Please Subscribe.If you love the article, Please consider supporting me by buying a coffee for $1."
  },
  
  {
    "title": "SPARK Interview Questions Beginner PART-1",
    "url": "/posts/SparkInterviewquestions-Beginner-Part1/",
    "categories": "Interview Questions",
    "tags": "spark, dataengineer, interviewquestions",
    "date": "2023-11-30 12:00:00 +1100",
    





    
    "snippet": "1- What is Lazy Evaluation in Apache Spark?Before diving into the intricacies of lazy evaluation, it’s crucial to grasp two fundamental concepts in Spark: Transformation and Action.Transformation:I...",
    "content": "1- What is Lazy Evaluation in Apache Spark?Before diving into the intricacies of lazy evaluation, it’s crucial to grasp two fundamental concepts in Spark: Transformation and Action.Transformation:In the realm of Spark, the core data structures are immutable, meaning they cannot be altered once created. Transformations are the instructions for modifying these core data structures to shape the DataFrame into the desired form.A Spark operation that reads a DataFrame, manipulates some of the columns, and returns another DataFrame is referred to as a transformation. Noteworthy is the fact that transformations are evaluated in a lazy fashion. Regardless of the number of scheduled transformations, no Spark jobs are triggered until an action is invoked. Examples of transformations include map(), filter(), groupByKey(), reduceByKey(), join(), and union().Action:Spark actions are operations that prompt a Spark job to compute and return a result to the Spark driver program or write data to an external storage system. Unlike Spark transformations, which only define a computation path without execution, actions enforce Spark to compute and produce a tangible result. Examples of actions include count, collect, sum, max, min, and foreach.Lazy Evaluation:Lazy evaluation is a cornerstone feature of Apache Spark that enhances its efficiency and performance. It involves postponing the execution of transformations on distributed datasets until an action is called. This strategy ensures that Spark only processes data when absolutely necessary, leading to significant performance improvements.When performing operations on RDDs/DataFrames/DataSets, such as filtering or mapping, Spark refrains from immediate data processing. Instead, it constructs a logical execution plan, known as the Directed Acyclic Graph (DAG), which represents the sequence of transformations to be applied incrementally.The evaluation of the DAG commences only when an action is invoked. Some examples of actions in Spark include collect, count, saveAsTextFile, first, foreach, and countByKey.Advantages of Lazy Evaluation:  Optimization  Reduced Disk I/O and Memory UsageIn conclusion, lazy evaluation is a powerful feature of Apache Spark that significantly bolsters its performance and efficiency. Understanding this mechanism is essential for harnessing the full potential of Spark’s distributed data processing capabilities.If you enjoy the article, Please Subscribe.If you love the article, Please consider supporting me by buying a coffee for $1."
  },
  
  {
    "title": "Part 2 - Orchestrating Snowflake Data Transformations with DBT on Amazon ECS through Apache Airflow",
    "url": "/posts/snowflake_dbt_ecs_part2/",
    "categories": "PROJECT",
    "tags": "snowflake, dbt, ecs, mwaa, project",
    "date": "2023-10-24 12:00:00 +1100",
    





    
    "snippet": "Overview:In our previous post, we explored the setup of DBT on an ECR private repository through an AWS pipeline. In this blog, our emphasis will be on configuring MWAA and initiating DBT processes...",
    "content": "Overview:In our previous post, we explored the setup of DBT on an ECR private repository through an AWS pipeline. In this blog, our emphasis will be on configuring MWAA and initiating DBT processes using Amazon’s managed Apache Airflow (MWAA). Please find the source code on my GitRepo.Architecture:ArchitectureRefer to my previous blog for instructions on configuring DBT on an ECR Private Repository.PART 1MWAA:Amazon Managed Workflows allow developers the ability to quickly deploy an Airflow instance on AWS that utilises a combination of other AWS services to optimise the overall set-up.MWAASTEP 1: To execute DBT within Airflow, the initial step is to establish MWAA.Here are the steps for configuring MWAA:  1.Select the S3 bucket from which your MWAA will retrieve the Directed Acyclic Graph (DAG) files.MWAA S3 Bucket Config  2.Choose the Environment Class according to the number of DAGs run in your environment.MWAA ENV Config  3.IAM Role permission for Airflow:The following is a list of IAM permissions necessary to run our Airflow. Given that our Directed Acyclic Graph (DAG) is located in the S3 bucket, the MWAA role inherently has S3 bucket access.IAM Role for SSM:Additionally, in our DAG, we’ll be retrieving ECS cluster and Task details from the Parameter Store, which necessitates the required access.{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": \"ssm:GetParameter\",            \"Resource\": \"*\"        }    ]}IAM Role for ECS Task Execution: Considering that we are making calls to the ECR repository, please ensure that the Task Execution policy has the necessary permissions.{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": [                \"ecr:GetAuthorizationToken\",                \"ecr:BatchCheckLayerAvailability\",                \"ecr:GetDownloadUrlForLayer\",                \"ecr:BatchGetImage\",                \"logs:CreateLogStream\",                \"logs:PutLogEvents\"            ],            \"Resource\": \"*\"        }    ]}MWAA set up has been completed.STEP 2: To invoke DBT from Airflow, it is essential to configure the ECS Task definition and cluster.Set up ECS Cluster and Task definition:AWS ECSAWS ECS runningSTEP 3: Place your DAG Code in the S3 Bucket.from airflow import DAGfrom airflow.utils.dates import days_agofrom datetime import timedelta, datetimefrom airflow.providers.amazon.aws.operators.ecs import  EcsRunTaskOperatorimport boto3ssm = boto3.client('ssm')default_args={        \"start_date\": days_ago(2),        \"owner\": \"jayaananth\",        \"email\": [\"jayaananth@gmail.com\"],        \"retries\": 1,        \"retry_delay\" :timedelta(minutes=5)    }with DAG(\"dbt_scd2_snowflake_ecs_operator\", start_date=datetime(2022, 1 ,1),     schedule_interval=\"@daily\", default_args=default_args, catchup=False) as dag:    # Get ECS configuration from SSM parameters    ecs_cluster               = str(ssm.get_parameter(Name='/mwaa/ecs/cluster', WithDecryption=True)['Parameter']['Value'])    ecs_task_definition       = str(ssm.get_parameter(Name='/mwaa/ecs/task_definition', WithDecryption=True)['Parameter']['Value'])    ecs_subnets               = str(ssm.get_parameter(Name='/mwaa/vpc/private_subnets', WithDecryption=True)['Parameter']['Value'])    ecs_security_group        = str(ssm.get_parameter(Name='/mwaa/vpc/security_group', WithDecryption=True)['Parameter']['Value'])##ecs_awslogs_group         = str(ssm.get_parameter(Name='/mwaa/cw/log_group', WithDecryption=True)['Parameter']['Value'])#ecs_awslogs_stream_prefix = str(ssm.get_parameter(Name='/mwaa/cw/log_stream', WithDecryption=True)['Parameter']['Value'])    print(ecs_task_definition)# Run Docker container via ECS operator    task_model_ecs_operator = EcsRunTaskOperator(        task_id=\"snowflake_dbt_model_ecs_operator\",        dag=dag,        aws_conn_id=\"aws_default\",        cluster=ecs_cluster,        task_definition=ecs_task_definition,        launch_type=\"FARGATE\",        overrides={          \"containerOverrides\": [                {                  \"name\": \"jay-snowflake\",                  \"command\": [\"dbt\",\"run\",\"--select\", \"models/emp_fact.sql\"]                },            ],        },        network_configuration={           \"awsvpcConfiguration\": {                \"securityGroups\": [ecs_security_group],                \"subnets\": ecs_subnets.split(\",\") ,            },        },#        # awslogs_group=\"ecs_awslogs_group\",        #awslogs_stream_prefix=\"ecs_awslogs_stream_prefix\"    )        task_snapshot_ecs_operator = EcsRunTaskOperator(        task_id=\"ecs_snowflake_operator\",        dag=dag,        aws_conn_id=\"aws_default\",        cluster=ecs_cluster,        task_definition=ecs_task_definition,        launch_type=\"FARGATE\",        overrides={          \"containerOverrides\": [                {                  \"name\": \"jay-snowflake\",                  \"command\": [\"dbt\",\"snapshot\",\"--select\", \"snapshots/scd_emp.sql\"]                },            ],        },        network_configuration={            \"awsvpcConfiguration\": {                \"securityGroups\": [ecs_security_group],                \"subnets\": ecs_subnets.split(\",\") ,            },        },        # awslogs_group=\"ecs_awslogs_group\",        #awslogs_stream_prefix=\"ecs_awslogs_stream_prefix\"    )task_model_ecs_operator.set_downstream(task_snapshot_ecs_operator)In the container override section, we will provide the ECR image name and specify the command to execute when Airflow triggers the job.The DAG will retrieve the Task definition and cluster information from the Systems Manager (SSM) Parameter Store.AWS SSMSTEP 4: Trigger your DAG.MWAA DAG JobIn the image below, you can see your DAG executing the ECS Task function.AWS ECS Task FunctionExample:In the below example, we will capture the result pre and post after airflow jobs.Source:Source Table EmployeeSource Table DepartmentSource Table Employee_DepartmentTarget:Target Emp_fact TableTarget Emp_fact TableUpdate the source record to capture the result.UPDATE employee SET LAST_NAME='JAYARAM', updated_at=CURRENT_TIMESTAMP() WHERE emp_no=1; Target post run:Post Target Emp_fact TablePost Target SCD_fact TablePost Target SCD_fact TableAdvantages of Scheduled DBT Model Deployment for Snowflake Utilising AWS ECS and Airflow:      Automation: Scheduled deployments in AWS ECS and Airflow automate the process of running DBT models, reducing manual intervention and minimising the risk of errors.        Efficiency: Automation saves time and resources, making it more efficient to manage and update your DBT models, which can be particularly useful when dealing with large datasets.        Monitoring: Airflow provides monitoring and logging capabilities, allowing you to track the progress and performance of your DBT tasks, making it easier to troubleshoot issues.        Scheduling Flexibility: You can schedule DBT runs during off-peak hours or based on business requirements, ensuring that the data transformation processes do not impact regular operations.        Error Handling: Airflow enables you to set up error-handling mechanisms, such as notifications or retries, to ensure that your DBT tasks are robust and resilient.  Conclusion:In this blog, we have explored the construction of Snowflake Data Transformations using DBT on Amazon ECS within the context of Apache Airflow. This approach offers a versatile and all-encompassing structure for organisations to improve their data transformation processes.Note: This article was originally published on  Cevo Australia’s website If you enjoy the article, Please Subscribe.If you love the article, Please consider supporting me by buying a coffee for $1."
  },
  
  {
    "title": "Part 1 - Orchestrating Snowflake Data Transformations with DBT on Amazon ECS through Apache Airflow.",
    "url": "/posts/snowflake_dbt_ecs_part1/",
    "categories": "PROJECT",
    "tags": "snowflake, dbt, ecs, mwaa, project",
    "date": "2023-10-24 12:00:00 +1100",
    





    
    "snippet": "Overview:This blog offers a comprehensive walkthrough for setting up DBT to execute data transformation tasks specifically designed for Snowflake. We’ve streamlined the DBT configuration process by...",
    "content": "Overview:This blog offers a comprehensive walkthrough for setting up DBT to execute data transformation tasks specifically designed for Snowflake. We’ve streamlined the DBT configuration process by packaging it within a Docker Image, which is securely stored in a private ECR repository. To efficiently handle scheduling and orchestration, we’ve harnessed the power of both ECS Service and MWAA. You can access the source code in this GitRepo.Architecture:ArchitectureData Build Tool:Data build tool (DBT) enables analytics engineers to transform data in their warehouses by simply writing select statements. DBT handles turning these select statements into tables and views.DBT does the T in ELT (Extract, Load, Transform) processes – it doesn’t extract or load data, but it’s extremely good at transforming data that’s already loaded into your warehouse.Infrastructure :There are various installation methods for DBT, but in our case, where our aim is to deploy DBT as a container on ECS Fargate, the initial step entails Dockerfile preparation.Docker File: # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: MIT-0FROM python:3.8-slim-busterADD dbt-project /dbt-project# Update and install system packagesRUN apt-get update -y &amp;&amp; \\  apt-get install --no-install-recommends -y -q \\  git libpq-dev python-dev &amp;&amp; \\  apt-get clean &amp;&amp; \\  rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*# Install DBTRUN pip install -U pipRUN pip install dbt-coreRUN pip install dbt-snowflakeRUN mkdir -p /root/.dbtCOPY /dbt-project/profiles.yml /root/.dbt/profiles.ymlWORKDIR /dbt-project#CMD [\"/bin/bash\"]#ENTRYPOINT [\"dbt\",\"run\"]DBT Project:A DBT project is a directory of .sql and .yml files, which DBT uses to transform your data. At a minimum, a DBT project must contain:      A project file: A DBT_project.yml file tells DBT that a particular directory is a DBT project, and also contains configurations for your project.        Models: A model is a single .sql file. Each model contains a single select statement that either transforms raw data into a dataset that is ready for analytics, or, more often, is an intermediate step in such a transformation.A project may also contain a number of other resources, such as snapshots, seeds, tests, macros, documentation, and sources.  Source Code:The source code is housed within the CodeCommit repository. Please find the source code GitRepo.Please find the tree of the source code, as seen below.-Config  -buildspec.yml-SourceCode  -dbt-project    - dbt_project.yml    - README.md    - tests    - snapshots    - models    - macros    - data    - analysis -DockerfileCode Commit SrcDBT configuration:In this instance, we are establishing a connection between the DBT tool and Snowflake. Typically, database connections are retrieved from the profiles.yml file. Meanwhile, sensitive information is dynamically extracted from Secret Manager or Parameter Store during the code building process.The details we have passed in Code Build as an environmental variable.dbtlearn:  target: dev        outputs:    dev:      type: snowflake      account: ********      user: dbt      password: **********      role: transform      database: customer      warehouse: compute_wh      schema: dev      threads: 1      client_session_keep_alive: FalseCode Build:The build process will dynamically retrieve the confidential database connection information from Secret Manager or Parameter Store. It will then proceed to construct the source code and subsequently push it as an image to the private ECR repository.Buildspec.ymlThe yaml file defines a series of commands to be executed during different build phases. It sets up a Python environment, configures Snowflake credentials, builds and tags a Docker image, and pushes it to an Amazon ECR repository in the ap-southeast-2 region.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: MIT-0version: 0.2phases:  install:    runtime-versions:      python: 3.11              pre_build:    commands:      - aws --version      - echo 'region - ' - $AWS_DEFAULT_REGION      - echo 'repository - ' $REPOSITORY_URI      - cd src/dbt-project/      - sed -i -e \"s/\\(type:\\).*/\\1 $SNOWFLAKE_TYPE/\" profiles.yml      - sed -i -e \"s/\\(account:\\).*/\\1 $SNOWFLAKE_ACCOUNT/\" profiles.yml      - sed -i -e \"s/\\(user:\\).*/\\1 $SNOWFLAKE_USER/\" profiles.yml      - sed -i -e \"s/\\(password:\\).*/\\1 $SNOWFLAKE_PASSWORD/\" profiles.yml      - sed -i -e \"s/\\(role:\\).*/\\1 $SNOWFLAKE_ROLE/\" profiles.yml      - sed -i -e \"s/\\(database:\\).*/\\1 $SNOWFLAKE_DATABASE/\" profiles.yml      - sed -i -e \"s/\\(warehouse:\\).*/\\1 $SNOWFLAKE_WAREHOUSE/\" profiles.yml      - sed -i -e \"s/\\(schema:\\).*/\\1 $SNOWFLAKE_SCHEMA/\" profiles.yml      - sed -i -e \"s/\\(threads:\\).*/\\1 $SNOWFLAKE_THREADS/\" profiles.yml      - cat profiles.yml      - cd ../      - echo Logging in to Amazon ECR      #- $(aws ecr get-login --region ap-southeast-2 --no-include-email)      - aws ecr get-login-password --region ap-southeast-2 | docker login --username AWS --password-stdin 590312749310.dkr.ecr.ap-southeast-2.amazonaws.com  build:    commands:      - echo Build started on `date`      - echo Building the Docker image...      - docker build -t ********.dkr.ecr.ap-southeast-2.amazonaws.com/jaydbt:latest .      - docker tag *******.dkr.ecr.ap-southeast-2.amazonaws.com/jaydbt:latest *******.dkr.ecr.ap-southeast-2.amazonaws.com/dbt:latest  post_build:    commands:      - echo Build completed on `date`      - echo Push the latest Docker Image...      - docker push 590312749310.dkr.ecr.ap-southeast-2.amazonaws.com/dbt:latestCode Build Environment Variable:Code Build ParamUp to this point, we’ve successfully compiled the code and uploaded our Docker image to the ECR Repository.ECRAs an illustrative example, we will be writing a code in DBT to implement Slowly Changing Dimensions (SCD) type 2 using models and snapshots in the Snowflake environment.DBT offers a feature called snapshots, which allows you to capture and track changes made to a table that can be updated over time. Snapshots are particularly useful for implementing type-2 SCDs on tables that can be modified.source/sources.ymlversion: 2sources:  - name: customer    schema: dev    tables:        - name: emp_fact          identifier: emp_fact        - name: emp          identifier: employeemodels/emp_fact.sqlselect employee.EMP_NO,employee.FIRST_NAME,employee.LAST_NAME,employee.GENDER,department.DEPT_NAME,employee.UPDATED_ATfrom employee join dept_empon dept_emp.emp_no=employee.emp_nojoin departmenton department.dept_no=dept_emp.dept_nosnapshots/scd_emp.sql{% snapshot scd_emp %}{{ config( target_schema='public', unique_key='emp_no', strategy='timestamp', updated_at='updated_at', invalidate_hard_deletes=True )}}select * FROM  {{ source('customer', 'emp') }} {% endsnapshot %}Summary:In this blog, we explored the process of configuring DBT on an ECR private repository via an AWS pipeline. In my next blog post, we will delve into the configuration of MWAA and demonstrate how to initiate the DBT job using Airflow.Note: This article was originally published on  Cevo Australia’s website If you enjoy the article, Please Subscribe.If you love the article, Please consider supporting me by buying a coffee for $1."
  },
  
  {
    "title": "Building Serverless PySpark Jobs with EMR-Serverless and MWAA",
    "url": "/posts/EMRserverless/",
    "categories": "EMR",
    "tags": "emr, spark, airflow, mwaa",
    "date": "2023-10-15 12:00:00 +1100",
    





    
    "snippet": "IntroductionAmazon Web Services (AWS) provides a rich ecosystem of tools and services to tackle big data challenges. Two powerful components that stand out are Amazon EMR (Elastic MapReduce) and Am...",
    "content": "IntroductionAmazon Web Services (AWS) provides a rich ecosystem of tools and services to tackle big data challenges. Two powerful components that stand out are Amazon EMR (Elastic MapReduce) and Amazon MWAA (Managed Workflows for Apache Airflow).In this blog, we will explore how the combination of EMR Serverless PySpark jobs on MWAA revolutionises big data processing and analysis.EMR Serverless Overview:With  EMRServerless , you pay only for the resources you consume while your jobs are running, and you have the option to pause your cluster when it is not in use to save on costs. EMR serverless also provides built-in integrations with other AWS services, such as Amazon S3 and Amazon DynamoDB, which can make it easier to store and access data for your big data workloads.MWAA Overview :MWAA stands for Managed Workflows for Apache Airflow  MWAA , which is a fully managed service provided by AWS. Apache Airflow is an open-source platform used for orchestrating, scheduling, and monitoring complex data workflows. It allows you to define, schedule, and manage data pipelines as directed acyclic graphs.MWAA simplifies the deployment and management of Apache Airflow environments. It handles the underlying infrastructure, including provisioning servers, scaling, patching and maintenance, so that you can focus on designing and running your data workflows.Implementation:By integrating EMR serverless PySpark jobs with MWAA, you can create sophisticated data processing pipelines that are orchestrated and managed seamlessly. MWAA’s support for dynamic scaling ensures that resources are allocated efficiently based on job requirements, further optimising cost and performance.PySpark CodeThe below PySpark code to read from the post gres employee table and insert into the other employee_v1table.from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(\"ETL Pipeline\").getOrCreate()# COMMAND ----------# Credentialdb_properties = {    \"user\": \"postgres\",    \"password\": \"******\",    \"driver\": \"org.postgresql.Driver\"}# Credentialpostgres_url = \"jdbc:postgresql://sparkemr.ctrfy31kg8iq.ap-southeast-2.rds.amazonaws.com/customer\"#Table Detailstable_name = \"employee\"table_schema = \"public\"# Read from Postgres tabledf = spark.read.jdbc(url=postgres_url, table=f\"{table_schema}.{table_name}\", properties=db_properties)df.show()table_name=\"employee_v1\"# Write  from Postgres tabledf.write.jdbc(url=postgres_url,                  table=f\"{table_schema}.{table_name}\",                  mode=\"append\",                  properties=db_properties) ```VPC Configuration using Terraformprovider \"aws\" {  region = \"ap-southeast-2\"  # Change to your desired AWS region}module \"vpc\" {  source  = \"terraform-aws-modules/vpc/aws\"  version = \"5.1.1\"  # VPC Basic Details  name = \"sparkemr-dev\"  cidr = \"10.0.0.0/16\"     azs = [\"ap-southeast-2a\", \"ap-southeast-2b\",\"ap-southeast-2c\"]  private_subnets     = [\"10.0.16.0/20\",                        \"10.0.32.0/20\",\"10.0.64.0/20\"]  public_subnets      = [\"10.0.0.0/20\",                           \"10.0.48.0/20\",\"10.0.80.0/20\"]  # Database Subnets  #create_database_subnet_group = true  #create_database_subnet_route_table= true  #database_subnets    = [\"10.0.151.0/24\", \"10.0.152.0/24\"]  #create_database_nat_gateway_route = true  #create_database_internet_gateway_route = true  # NAT Gateways - Outbound Communication  enable_nat_gateway = true  single_nat_gateway = true  # VPC DNS Parameters  enable_dns_hostnames = true  enable_dns_support = true  public_subnet_tags = {    Type = \"sparkemr-public-subnets\"  }  private_subnet_tags = {    Type = \"sparkemr-private-subnets\"  }  tags = {    Owner = \"Jay\"    Environment = \"dev\"  }  vpc_tags = {    Name = \"sparkemr-dev\"  }}MWAA DAGA Directed Acyclic Graph (DAG) is a graph object that represents a workflow in Airflow. It is a collection of tasks in a way that shows each task’s relationships and dependencies. DAGs contain the context of task execution. In MWAA, we will keep DAGs in S3.Step 1: Create the MWAA with required IAM policy and VPC dependencies.Step 2: Add the below DAG in the specified S3 bucket.The below DAG creates an EMR serverless application and runs the PySpark job on it. Once the job has been completed successfully the dag will delete the EMR applications.from datetime import datetime  from airflow import DAG  from airflow.providers.amazon.aws.operators.emr import (     EmrServerlessCreateApplicationOperator,     EmrServerlessStartJobOperator,     EmrServerlessDeleteApplicationOperator,   )   # Replace these with your correct values   JOB_ROLE_ARN = \"arn:aws:iam::*******:role/AmazonEMR-ExecutionRole-*******\"   S3_LOGS_BUCKET = \"jay-airflow\"   DEFAULT_MONITORING_CONFIG = {     \"monitoringConfiguration\": {       \"s3MonitoringConfiguration\": {\"logUri\":\"s3://**-airflow/logs/\"}     },   }   #   with DAG(     dag_id=\"emr_serverless_job\",     schedule_interval=None,     start_date=datetime(2023, 1, 1),     tags=[\"postgres\"],     catchup=False,   ) as dag:     network_config = {       'subnetIds': [        'subnet-0d80705cb360536b5','subnet-0c84b6541e43ee23b','subnet-018773ddec10cd902'       ],       'securityGroupIds': [         'sg-0357c733a2afebcc1',       ]     }     create_app = EmrServerlessCreateApplicationOperator(       task_id=\"create_spark_app\",       job_type=\"SPARK\",       release_label=\"emr-6.7.0\",       config={\"name\": \"EMR-Airflow\"           ,\"networkConfiguration\": network_config           },     )     application_id = create_app.output     job1 = EmrServerlessStartJobOperator(       task_id=\"start_job_1\",       application_id=application_id,       execution_role_arn=JOB_ROLE_ARN,       job_driver={         \"sparkSubmit\": {           \"entryPoint\": \"s3://***-emr/sparkemr.py\",           \"entryPointArguments\": [\"s3://jay-emr/outputs\"],           \"sparkSubmitParameters\": \"--conf   spark.jars=s3://jay-emr/postgresql-42.6.0.jar --conf spark.executor.cores=1 --conf spark.executor.memory=4g --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1\"         }       },       configuration_overrides=DEFAULT_MONITORING_CONFIG,     )     delete_app = EmrServerlessDeleteApplicationOperator(       task_id=\"delete_app\",       application_id=application_id,       trigger_rule=\"all_done\",     )     create_app &gt;&gt; job1 &gt;&gt; delete_appStep 3: Open the MWAA user interface (UI) and find the below DAG.MWAA AirflowStep 4 :In the demo we are manually triggering the job.The create_spark_app will create the EMR Application.Start job will run the spark job on the EMR applications.EMR PysparkDelete_app will delete the EMR application which was created as a part of the first step once the spark job has completed.EMR PysparkResultEMR ResultBenefits of combining EMR serverless PySpark jobs with MWAA:Cost efficiency: By leveraging serverless PySpark jobs on EMR and dynamic scaling of MWAA, you can achieve cost savings by only using resources when needed. This pay-as-you-go model eliminates the need for over-provisioning clusters, reducing operational costs.Simplicity: The combination of EMR serverless and MWAA eliminates much of the complexity associated with cluster provisioning and management. This simplicity allows data engineers and analysts to focus more on data processing logic and less on infrastructure concerns.Scalability: Both EMR and MWAA are designed to scale seamlessly based on workload demands. As your data processing requirements grow, you can trust that the underlying infrastructure will adapt accordingly.Workflow Orchestration: MWAA’s integration with Apache Airflow provides a robust framework for building, scheduling and monitoring complex data workflows. This orchestration capability ensures that your EMR serverless PySpark jobs are executed in a structured and controlled manner.ConclusionIn this post we have discussed how to integrate PySpark and EMR serverless schedules in MWAA.The combination of Amazon EMR serverless PySpark jobs and Amazon MWAA for workflow orchestration offers a powerful solution for processing and analysing big data in a flexible, scalable and cost-effective manner. This duo empowers organisations to focus on extracting valuable insights from their data without the hassle of infrastructure management.Note: This article was originally published on  Cevo Australia’s website If you enjoy the article, Please Subscribe.If you love the article, Please consider supporting me by buying a coffee for $1."
  },
  
  {
    "title": "Continuous Data Ingestion with Snowpipe and Stream in Snowflake",
    "url": "/posts/snowpipe/",
    "categories": "Snowflake",
    "tags": "snowflake, snowflake snowpipe, snowflake stream, cdc, snowflake task",
    "date": "2023-10-15 12:00:00 +1100",
    





    
    "snippet": "IntroductionSnowflake data pipelines offer significant benefits to organisations by streamlining and automating data processing workflows. Snowflake pipelines revolutionise data management and empo...",
    "content": "IntroductionSnowflake data pipelines offer significant benefits to organisations by streamlining and automating data processing workflows. Snowflake pipelines revolutionise data management and empower organisations to derive valuable insights from their data assets in a more efficient and timely manner.In this blog, we’ll explain how to create a Snowflake data pipeline to automate the manual processes involved in creating and managing ELT logic for transforming and improving continuous data loads.DefinitionsSnowpipe  Snowpipe  is a cloud-native, real-time data ingestion service provided by Snowflake. It allows you to load data into Snowflake tables automatically as new data arrives in your cloud storage (e.g., Amazon S3 or Azure Blob Storage).Change Data CaptureChange Data Capture  CDC   is a technique used to capture and propagate changes made to a database in real-time. CDC identifies and captures data changes, such as inserts, updates, and deletes, from a source system’s transaction log. It provides a reliable and efficient way to track data changes and replicate them to target systems.Stream A stream  data manipulation language (DML) changes made to a table, directory table, external table, or the underlying tables in a view (including secure views). The object for which changes are recorded is called the source object.ArchitectureThe following diagram represents the architecture of the Snowflake data pipeline.Snowpipe ArchitectCreating a Snowflake data pipelineTo establish uninterrupted data pipelines, we will leverage the following Snowflake features:  Utilising an external stage on Amazon S3  Employing SnowPipe, a feature within Snowflake  Leveraging streams functionality  Utilising tasks to enable continuous data pipelines  Implementing stored procedures to support the uninterrupted flow of data.Step 1: Create an IAM Role for Snowflake to access data in S3 bucketCreating an IAM Role specifically for Snowflake to access the S3 bucket ensures secure, controlled, and auditable access to the data.IAM RoleStep 2: Create an AWS S3 bucket and upload sample files.This step involves setting up an external stage on Amazon S3, a storage service provided by Amazon Web Services. This stage serves as a destination for receiving input files that will be used in the data processing workflow.Once the input files are placed in the designated S3 bucket, Snowpipe, a specialised feature within Snowflake, is triggered. Snowpipe automatically detects new files added to the S3 bucket and initiates the subsequent steps in the data pipeline.AWS S3 BucketStep 3: Create an integration object in Snowflake. Storage integration  is a Snowflake object that stores a generated identity and access management (IAM) entity for your external cloud storage, along with an optional set of allowed or blocked storage locations (Amazon S3, Google Cloud Storage, or Microsoft Azure).CREATE STORAGE INTEGRATION aws_sf_data_load  TYPE = EXTERNAL_STAGE  STORAGE_PROVIDER = S3  ENABLED = TRUE  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::xxxxxxxxxxx:role/snowflake-aws-jay'  STORAGE_ALLOWED_LOCATIONS = ('s3://snowflake-continousingestion/');Snowflake’s External_ID and IAM User ARN are required to access AWS.desc INTEGRATION aws_sf_data_load;Description TableUnder ‘trust relationships’ within the IAM Role, add External_ID and IAM User ARN.External_IDStep 4:In Snowflake, construct a file format object. A file format  is a configuration that specifies how data is organised and structured within files that are loaded into or exported from Snowflake tables.CREATE FILE FORMAT csv_load_format    TYPE = 'CSV'     COMPRESSION = 'AUTO'     FIELD_DELIMITER = ','     RECORD_DELIMITER = '\\n'     SKIP_HEADER =1     FIELD_OPTIONALLY_ENCLOSED_BY = '\\042'     TRIM_SPACE = FALSE     ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE     ESCAPE = 'NONE'     ESCAPE_UNENCLOSED_FIELD = '\\134'     DATE_FORMAT = 'AUTO'     TIMESTAMP_FORMAT = 'AUTO';Step 5: Create a stage object in Snowflake. A stage  specifies where data files are stored (i.e. “staged”) so that the data in the files can be loaded into a table. A named stage is a cloud storage location managed by Snowflake. Creating a named stage is useful if you want multiple users or processes to upload files.CREATE stage stg_lairport_details_csv_devstorage_integration = aws_sf_data_loadurl = 's3://snowflake-continousingestion/airport_details/csv/'file_format = csv_load_format;Step 6:  Create a Snowpipe.We can now develop snowpipe to automatically ingest data from S3 to Snowflake because we have defined stage, table, and file format. The snowpipe will pick up and load a new file into the raw table whenever a new file is placed in the bucket.create or replace pipe airport_details_pipe auto_ingest=true ascopy into airport_raw_details from @stg_airport_details_csv_dev ON_ERROR = continue;Description TableSubscribe the Snowflake SQS Queue in s3:. To do this:  Log in to AWS Console  Click on properties  Click on create event notification  Type event name (prefix and suffix are optional)  Select all object create events  Select SQS queue in destination → Select “enter SQS queue ARN” in the Specify SQS queue  Paste the ARN(notification_channel) copied from Snowflake in the SQS queue box  Save changesEvent NotificationDestinationsSummaryStep 7: Create a stream on the raw table.CREATE OR REPLACE STREAM airport_std_stream ON TABLE airport_raw_details;For any new DML operations performed in the table, the changes will be captured in the stream table as below.select * from airport_std_stream;Table ResultStep 8:Create a task.In Snowflake,  a task  is a feature that enables the automation of various actions and processes within the Snowflake data warehouse. It allows you to schedule and execute a series of SQL statements or stored procedures as a single unit.create or replace task airport_load_tsk warehouse = compute_whschedule = '1 minute'when system$stream_has_data('airport_std_stream')as merge into airport_prod_details as li using (   select         AIRLINE_ID,        AIRLINE_DESCRIPTION            from         airport_std_stream    where metadata$action='INSERT') as li_stgon li.AIRLINE_ID = li_stg.AIRLINE_ID when matched then update set     li.AIRLINE_ID = li_stg.AIRLINE_ID,    li.AIRLINE_DESCRIPTION = li_stg.AIRLINE_DESCRIPTIONwhen not matched then insert (    AIRLINE_ID,    AIRLINE_DESCRIPTION) values (    li_stg.AIRLINE_ID,    li_stg.AIRLINE_DESCRIPTION);Making use of the altered data captured in streams, you can create tasks to execute SQL queries or procedures. In accordance with a timetable for running SQL statements, including those that invoke stored procedures, you can schedule tasks to execute repeatedly.Instead of always using the entire source table, we may utilise the TASK’s MERGE statement to MERGE only the changes (streams) from the source table to the target table.ALTER TASK airport_load_tsk RESUME;EXECUTE TASK airport_load_tskNext, validate the result.Once Streams capture any data change on the source table(AIRPORT_RAW_DETAILS). So all the new data added to the Target table AIRPORT_PROD_DETAILS.Prod DetailsConclusionIn this post, we have discussed how to create the Snowflake data pipeline. By implementing Snowflake data pipelines, organisations can unlock the full potential of their data assets, enabling efficient and automated data processing, real-time insights, improved data quality, cost optimisation, and enhanced data governance. These benefits contribute to informed decision-making, improved operational efficiency, and a competitive advantage in the data-driven business landscape.Note: This article was originally published on  Cevo Australia’s website If you enjoy the article, Please Subscribe.If you love the article, Please consider supporting me by buying a coffee for $1."
  },
  
  {
    "title": "Introduction to DBT Tool & Install and Running in EC2 machine",
    "url": "/posts/dbtec2/",
    "categories": "DBT",
    "tags": "dbt, snowflake, ec2",
    "date": "2023-10-06 12:00:00 +1100",
    





    
    "snippet": "What is DBT?DBT  (data build tool) enables analytics engineers to transform data in their warehouses by simply writing select statements. DBT handles turning these select statements into tables and...",
    "content": "What is DBT?DBT  (data build tool) enables analytics engineers to transform data in their warehouses by simply writing select statements. DBT handles turning these select statements into tables and views.DBT does the T in ELT (Extract, Load, Transform) processes – it doesn’t extract or load data, but it’s extremely good at transforming data that’s already loaded into your warehouse.There’s two main ways of working with DBTDBT Core is the software that takes a DBT project (.sql and .yml files) and a command and then creates tables/views in your warehouse. DBT Core includes a command line interface (CLI) so that users can execute DBT commands using a terminal program. DBT Core is open source and free to use.DBT Cloud is an application that helps teams use DBT. DBT Cloud provides a web-based IDE to develop DBT projects, a purpose-built scheduler, and a way to share DBT documentation with your team. DBT Cloud offers a number of features for free, as well as additional features in paid tiers (check out the pricing here).Database Connections​DBT connects to your data warehouse to run data transformation queries. As such, you’ll need a data warehouse with source data loaded in it to use DBT. DBT natively supports connections to Snowflake, BigQuery, Redshift and Postgres data warehousesProject:A DBT project is a directory of .sql and .yml files, which DBT uses to transform your data. At a minimum, a DBT project must contain:  A project file: A DBT_project.yml file tells DBT that a particular directory is a DBT project, and also contains configurations for your project.  Models: A model is a single .sql file. Each model contains a single select statement that either transforms raw data into a dataset that is ready for analytics, or, more often, is an intermediate step in such a transformation.A project may also contain a number of other resources, such as snapshots, seeds, tests, macros, documentation, and sources.Developing locally with the Command Line Interface (CLI)​To use the CLI:Prerequisite - Create the EC2 MachineSteps to Set up DBT core(i) Install DBT in Amazon EC2 Machine. For this demo installing DBT-snowflakeInorder to install DBT we need pip so first install pip→ sudo apt update→ sudo apt install python3-pipInstall the DBT with adapter pip install \\  DBT-core \\  DBT-postgres \\  DBT-redshift \\  DBT-snowflake \\  DBT-bigquery pip install DBT-snowflakeVerify the DBT versiondbt -versionResult:(ii) Initialise the DBT init (DBT init [project-name])and provide the connection details or it can be manually updated too (/root/.DBT/profiles.yml)dbt initResult:(iii) After the DBT init command the list of module folder will be createdls -ltrResult:(iv) To verify the credential details go to profile.yml filecd /root/.DBTcat profiles.ymlResult:(v) run DBT debug to verify the connectivityCommand:dbt debugResult:(vi) Verify the dbt_profile .yml file contains all the details of the projectCommand:cat dbt_profile.ymlResult:Model:A model is a select statement. Models are defined in .sql files (typically in your models directory):Each .sql file contains one model / select statementThe name of the file is used as the model nameModels can be nested in subdirectories within the models directoryWhen you execute the DBT run  command, DBT will build this model in your data warehouse by wrapping it in a create view as or create table as statement.Materializations:Materializations are strategies for persisting DBT models in a warehouse. There are four types of materializations built into DBT. They are:  table  view  incremental  ephemeral(vii) Under the model please write the Select statement and declare the materialised as view or tablecat my_first_dbt_model.sql    with source_data as (    select 1 as id, 'NJ' as state, '2020-02-01 00:01:00.000'::timestamp as updated_at    union all    select null as id, 'CT' as state, '2020-01-01 00:00:00.000'::timestamp as updated_at    union all    select 4 as id, 'VT' as state, '2020-01-01 00:00:00.000'::timestamp as updated_at)select *from source_data(vii) Run DBT run command to execute theCommand:dbt runResult:(viii) Verify the first and second model created in the snowflake.Select * from \"ANALYTICS\".\"DBT\".\"MY_FIRST_DBT_MODEL\";SummaryIn this post, we discussed how to set up a DBT on an EC2 instance and connect to the snowflake and implemented a simple transformation logic by creating a new table in the Database.Use Case:DBT tool is used to Create the table or view based on the select query.With DBT, data teams work directly within the warehouse to produce trusted datasets for reporting, ML modeling, and operational workflows.If you enjoy the article, Please Subscribe.If you love the article, Please consider supporting me by buying a coffee for $1."
  },
  
  {
    "title": "Dynamic Table in Snowflake : Implementing Type 2 Slowly Changing Dimensions (SCD) with Flexibility and Efficiency",
    "url": "/posts/snowflakedynamictable/",
    "categories": "Snowflake",
    "tags": "snowflake, snowflake dynamictable, scd2",
    "date": "2023-09-27 12:00:00 +1000",
    





    
    "snippet": "Introduction:Snowflake introduced ‘Dynamic Tables’ as a new feature in 2022. Dynamic tables can significantly simplify data engineering in Snowflake by providing a reliable, cost-effective, and aut...",
    "content": "Introduction:Snowflake introduced ‘Dynamic Tables’ as a new feature in 2022. Dynamic tables can significantly simplify data engineering in Snowflake by providing a reliable, cost-effective, and automated way to transform data for consumption.In this blog, we will discuss what a dynamic table is, when to use it and we’ll also build SCD type 2 using the dynamic table with an example.Dynamic Table:Dynamic tables are a new table type in Snowflake that lets teams use simple SQL statements to declaratively define the result of data pipelines. They also automatically refresh as the data changes, only operating on new changes since the last refresh. The scheduling and orchestration needed to achieve this are also transparently managed by Snowflake.Comparison of materialised views with dynamic tablesMaterialised view vs dynamic table:Dynamic tables come with certain advantages, use cases and of course, challenges.Advantages:  Flexibility: Dynamic tables offer flexibility in that they can alter their structure or schema on the fly. Depending on particular circumstances or occurrences, columns may be added, modified, or eliminated.  Data Structures: Dynamic tables can handle structured and unstructured data.  Real Time Data: Dynamic tables can be updated in real-time as new data arrives, so the latest information is available immediately.Disadvantages:  Queries: Querying dynamic tables may require more complex and flexible query logic to handle the changing schema or structure of the data.  Data Integrity: It can be more difficult to ensure data integrity and maintain consistency when dynamic tables’ schema can vary, especially when relational constraints are involved.While materialised views have other strengths, weakness and use cases:Advantages:  Performance: Materialised views are pre-computed and stored copies of data that originate from one or more source tables. They are used to optimise query performance by reducing the need for expensive calculations or query-time joins.Disadvantages:  Fixed structure: Adding or removing columns in materialised views usually requires a rebuild of the view.  Data Consistency: In order to guarantee data consistency with the source tables, materialised views are often refreshed on a frequent basis. The automatic maintenance of materialised views consumes credits.Section 1: Preparing Your Data StructureCreate the EMPLOYEE Table and insert the sample data.CREATE OR REPLACE TABLE EMPLOYEE (  EMPLOYEE_ID INT,  FIRST_NAME VARCHAR(50),  LAST_NAME VARCHAR(50),  EMAIL VARCHAR(100),  PHONE_NUMBER VARCHAR(15),  FULL_ADDRESS VARCHAR(365),  UPDATE_TIME TIMESTAMP_NTZ(9));INSERT INTO EMPLOYEEVALUES(2, 'Roopa', 'Venkat', 'roopa.venkat@example.com', '0987654321', '456 Pine St, Sydney, NSW, 2170', '2023-05-25 10:00:00'),(1, 'Jay', 'J', 'jay.j@example.com', '1234567890', '789 Broadway St, Sydney, NSW, 2145', '2023-05-25 11:00:00'),(3, 'Rene', 'Essomba', 'rene.essomba@example.com', '1122334455', '321 Elm St, NSW, 2150', '2023-05-25 12:00:00');Section 2: Using SQL to Create Dynamic TablesA dynamic table is created to track the time a record was active and to serve as a substitute key for a specific version of a customer’s data. A Type 2 SCD history table, this one. The surrogate key “EMPLOYEE_HISTORY_SK,” which is specific to each record in the table, can be used for historical analysis and to join data to a particular version of a customer’s information.CREATE OR REPLACE DYNAMIC TABLE EMPLOYEE_HISTORYTARGET_LAG='1 MINUTE'WAREHOUSE=COMPUTE_WHASSELECT * RENAME (UPDATE_TIME AS RECORD_START_TIME),EMPLOYEE_ID || '-' || DATE_PART(EPOCH_MILLISECONDS, UPDATE_TIME) AS EMPLOYEE_HISTORY_SK,SPLIT_PART(FULL_ADDRESS, ' ', -1) AS POSTAL_CODE, CASE    WHEN LEAD(UPDATE_TIME) OVER (PARTITION BY EMPLOYEE_ID ORDER BY UPDATE_TIME ASC) IS NULL THEN '9999-12-31 12:00:00' -- Replace with a valid NULL date representation in your database    ELSE LEAD(UPDATE_TIME) OVER (PARTITION BY EMPLOYEE_ID ORDER BY UPDATE_TIME ASC)  END AS LEAD_TIMEFROM EMPLOYEE;A maximum lag of one minute is the aim set for the dynamic table. This ensures that data will never be delayed for longer than one minute, assuming proper warehouse provisioning.The current row’s update time is renamed as “RECORD_START_TIME” column, while the update time for the customer’s subsequent chronological row is listed in the “RECORD_END_TIME” column.“RECORD_END_TIME” is set to max date, indicating that the record includes the most recent information about the client. The dynamic table can also be used to calculate any derived columns, such as “POSTAL_CODE” in this example.Section 3 : Update the source records and verify the dynamic tableINSERT INTO EMPLOYEEVALUES(1, 'Jay', 'Jayaram', 'jay.jayaram@example.com', '1234567890', '789 Broadway St, Sydney, NSW, 2160', '2023-05-25 15:00:00')Source Result:Target Result:Streams and Tasks vs Dynamic TablesStreams and tasks versus dynamic tablesWith dynamic tables, data is received and altered from the base tables in response to a written query that specifies the desired outcome. The frequency of these refreshes is determined by an automatic refresh procedure, ensuring that the data satisfies the set freshness (lag) target. While joins, aggregations, window functions, and other SQL constructs are permissible in the SELECT statement for dynamic tables, calls to stored procedures, tasks, UDFs, or external functions are not permitted.However, Streams and Tasks provide additional flexibility. You can use Python, Java, or Scala-written user-defined functions (UDFs), user-defined table functions (UDTFs), stored procedures, external functions, and Snowpark transformations.Furthermore, streams and tasks offer flexibility in managing dependencies and scheduling, which makes them perfect for handling real-time data synchronisation, notifications, and task triggering with complete process control.Check out my previous blog to see how Stream and Task can be implementedDynamic tables are best used when:In certain circumstances, it could be required to process data where its structure needs to be flexible or doesn’t fit into a predetermined format. Dynamic tables come into play in this situation. Dynamic tables provide you more control over handling changing or unpredictable data structures.Use cases for dynamic tables include the following:Removes the need to manage data refreshes, track data dependencies, or write one’s own code.Reduces the complexity of stream and job.Doesn’t require precise control over the refresh schedule.Materialisation of the results of a query of multiple base tables.To summarise, materialised views are fantastic for improving the efficiency of repeated queries, but dynamic tables are perfect for building SQL-based data pipelines. A great option for tracking and controlling real-time data changes is streams and tasks.ConclusionIn this post, we have discussed how to create the dynamic table and implement SCD Type 2 using it. With dynamic tables, managing SCDs becomes simpler and more efficient compared to traditional methods.Note: This article was originally published on  Cevo Australia’s website If you enjoy the article, Please Subscribe.If you love the article, Please consider supporting me by buying a coffee for $1."
  },
  
  {
    "title": "Snowflake Dynamic Data Masking: Enhancing Data Security and Compliance",
    "url": "/posts/dynamicmasking/",
    "categories": "Snowflake",
    "tags": "snowflake, dynamic data masking, data security",
    "date": "2023-09-05 12:00:00 +1000",
    





    
    "snippet": "Introduction:Snowflake, a leading cloud data warehousing platform, offers a powerful feature called Dynamic Data Masking that plays a crucial role in enhancing data security, compliance, and data g...",
    "content": "Introduction:Snowflake, a leading cloud data warehousing platform, offers a powerful feature called Dynamic Data Masking that plays a crucial role in enhancing data security, compliance, and data governance.This blog post will delve into what Dynamic Data Masking is, its benefits, and how to implement it effectively within your Snowflake environment.What is Dynamic Data Masking?Dynamic Data Masking is a column-level security feature that uses masking policies to selectively mask plain-text data in tables and view columns at query time.In Snowflake, masking policies are schema-level objects, which means a database and schema must exist in Snowflake before a masking policy can be applied to a column. Currently, Snowflake supports using Dynamic Data Masking on tables and views.At query runtime, the masking policy is applied to the column at every location where the column appears. Depending on the masking policy conditions, the SQL execution context, and role hierarchy, Snowflake query operators may see the plain-text value, a partially masked value, or a fully masked value.Steps to apply Snowflake Dynamic Data Masking on a columnStep 1: Create a Custom Role with Masking PrivilegesThe below SQL statement creates a custom role TECHNICAL_LEAD in Snowflake.create role TECHNICAL_LEAD;grant usage on warehouse compute_wh to role TECHNICAL_LEAD;grant usage on database MARKETING_WH to role TECHNICAL_LEAD;grant usage on schema MARKETING_WH.PUBLIC to role TECHNICAL_LEAD;grant select on MARKETING_WH.PUBLIC.EMPLOYEE_DETAILS  to role TECHNICAL_LEAD;Step 2: Create a Masking PolicyThe below SQL statement creates a masking policy employee_dynamic_masking that can be applied to columns of type number.create or replace masking policy employee_dynamic_masking as (val NUMBER) returns number -&gt;  case    when current_role() in ('TECHNICAL_LEAD') then 99999999999999999999    else val  end;Terraform:terraform {  required_providers {    snowflake = {      source  = \"chanzuckerberg/snowflake\"      version = \"0.25.18\"    }  }}resource \"snowflake_masking_policy\" \"employee_dynamic_masking\" {  name               = \"employee_dynamic_masking\"  database           = \"MARKETING_WH\"  schema             = \"PUBLIC\"  value_data_type    = \"number\"  masking_expression = &lt;&lt;-EOFcasewhen current_role() in ('TECHNICAL_LEAD') thenvalelse'999999999999'endEOF  return_data_type = \"number\"}Step 3: Apply (Set) the Masking Policy to a Table or View ColumnALTER TABLE IF EXISTS \"MARKETING_WH\".\"PUBLIC\".employee_detailsMODIFY COLUMN SALARY SET MASKING POLICY employee_dynamic_masking;Step 4: Verify the masking rules by querying dataVerify the data present in the EMPLOYEE_DETAILS table by querying from two different roles.The below image shows data present in EMPLOYEE_DETAILS when queried from TECHNICAL_LEAD role.Benefits of Dynamic Data Masking  Data security: Dynamic Data Masking ensures that sensitive data remains confidential and is only accessible by authorised individuals.  Regulatory compliance: Dynamic Data Masking assists organisations in adhering to data protection regulations such as GDPR, HIPAA and more.Snowflake supports customers with IRAP(Australia) compliance requirement IRAP.  Fine-grained access control: Different users or roles can have varying levels of access to masked data, based on their authorisation level.ConclusionIn this post, we have discussed how to create the data masking manually and using Terraform. Dynamic Data Masking is a powerful feature that allows organisations to protect sensitive data while still enabling data analysis and sharing. By implementing Dynamic Data Masking in your Snowflake environment, you can enhance data security, meet compliance requirements, and maintain the trust of your customers and stakeholders.Note: This article was originally published on  Cevo Australia’s website If you enjoy the article, Please Subscribe.If you love the article, Please consider supporting me by buying a coffee for $1."
  }
  
]

